{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import balanced_accuracy_score, accuracy_score, roc_auc_score, f1_score\n",
    "\n",
    "import pickle\n",
    "from random import choices\n",
    "from itertools import product\n",
    "\n",
    "from gpytorch.models import ApproximateGP\n",
    "from gpytorch.variational import CholeskyVariationalDistribution\n",
    "from gpytorch.variational import VariationalStrategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Necessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function grid_generator generates a grid space used to find the next point to label based on uncertainty measures.\n",
    "# refer to the paper for more info on the input variables.\n",
    "def grid_generator():\n",
    "    tool = ['TOOLING-Aluminium-606x-Null-v1', # Tool material\n",
    "            'TOOLING-Composite-AS4-v1',\n",
    "            'TOOLING-Steel-1020-Null-v1',\n",
    "            'TOOLING-Invar-36-Null-v2']\n",
    "    top_htc = np.round(np.linspace(10,200,10)) # Top side htc\n",
    "    bottom_htc = np.round(np.linspace(5,150,10)) #  Bottom side htc\n",
    "    tool_thick = np.round(np.linspace(1,30,10)) # Tool thickness (mm)\n",
    "    mat_thick = np.round(np.linspace(1,30,10)) # Part thickness (mm)\n",
    "    ramp1 = np.round(np.linspace(1,5,5)) # Ramp 1 heating rate (C/min)\n",
    "    ramp2 = np.round(np.linspace(1,5,5)) # Ramp 2 heating rate (C/min)\n",
    "    hold1 = np.round(np.linspace(105,125,5)) # Hold time (C)\n",
    "\n",
    "    grid_np = np.array(list(product(tool, tool_thick, mat_thick, top_htc, bottom_htc, ramp1, hold1, ramp2)))\n",
    "    inp_col_names = ['tool', 'Tool_Thickness', 'Part_Thickness', 'Top_HTC', 'Bottom_HTC','Ramp1', 'Hold1_Temp', 'Ramp2']\n",
    "    grid = pd.DataFrame(grid_np, columns=inp_col_names)\n",
    "    return grid\n",
    "\n",
    "# The function takes the grid space and preprocess the data (one-hot encoding, normalization, etc.)\n",
    "def grid_preprocess(grid):\n",
    "    # One-hot encoder\n",
    "    encoder = OneHotEncoder()\n",
    "    toolMat = grid.iloc[:,0]\n",
    "    toolMat_1hot = encoder.fit_transform(toolMat.values.reshape(-1, 1))\n",
    "    toolMat_df = pd.DataFrame(toolMat_1hot.toarray(), columns=['ToolMat_1', 'ToolMat_2', 'ToolMat_3', 'ToolMat_4'])\n",
    "    #data_1hot = pd.concat([toolMat_df, data], axis=1)\n",
    "    data_concat = pd.concat([toolMat_df.reset_index(), grid.reset_index()], axis=1)\n",
    "    data_concat.drop(['index', 'tool'], axis=1, inplace=True)\n",
    "    \n",
    "    # Scale the data\n",
    "    scaler = StandardScaler()\n",
    "    data_scaled = scaler.fit_transform(data_concat)\n",
    "    \n",
    "    # Create mapping for onehot encoder\n",
    "    encoder_categories = ['TOOLING-Aluminium-606x-Null-v1', 'TOOLING-Composite-AS4-v1',\n",
    "                                        'TOOLING-Steel-1020-Null-v1',\n",
    "                                        'TOOLING-Invar-36-Null-v2']\n",
    "    encode_cc1 = [1, 2, 3, 4]\n",
    "    Tool_Mapping = {}\n",
    "    for i, j in zip(encode_cc1, encoder_categories):\n",
    "        Tool_Mapping[i] = j\n",
    "    \n",
    "    return data_scaled, scaler, Tool_Mapping, encoder\n",
    "\n",
    "# The function takes the raw data and prepares it for the training process\n",
    "def loadData(data, encoder):\n",
    "    toolMat = np.array(data['Tool_Material'])\n",
    "    toolMat_1hot = encoder.fit_transform(toolMat.reshape(-1, 1))\n",
    "    toolMat_df = pd.DataFrame(toolMat_1hot.toarray(), columns=['ToolMat_1', 'ToolMat_2', 'ToolMat_3', 'ToolMat_4'])\n",
    "    data_concat = pd.concat([toolMat_df.reset_index(), data.reset_index()], axis=1)\n",
    "    data_concat.drop(['index', 'Tool_Material'], axis=1, inplace=True)\n",
    "    data_concat.rename(columns={'Part_Max_Exotherm_Value': 'exotherm', 'Part_Max_Lag_Value': 'lag'}, inplace=True)\n",
    "    return data_concat, encoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Source network training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Generate the grid space and preprocess the data\n",
    "grid = grid_generator()\n",
    "data_processed, scaler, Tool_Mapping, onehot_encoder = grid_preprocess(grid)\n",
    "\n",
    "# 2) Load the one-hot encoder (alternatively one can use \"onehot_encoder\" from the code above).\n",
    "f = open('encoder.pickle', 'rb')\n",
    "onehot_encoder = pickle.load(f)\n",
    "\n",
    "# 3) Read data (train and validation)\n",
    "path = '\\Data\\\\'\n",
    "filename = '8552_Sample.csv'\n",
    "Main = pd.read_csv(path + filename, index_col=0)\n",
    "data, encoder = loadData(Main,onehot_encoder)\n",
    "X_train, X_valid, y_train, y_valid, scaler_s = dataprep(data, 0.2, refScaler=scaler)\n",
    "\n",
    "# 4) Source network architecture\n",
    "encoder = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=[11]),\n",
    "    tf.keras.layers.Dense(10, activation='selu', kernel_initializer='lecun_normal'),\n",
    "    tf.keras.layers.Dense(10, activation='selu', kernel_initializer='lecun_normal'),\n",
    "    tf.keras.layers.Dense(8, activation='selu', kernel_initializer='lecun_normal'),\n",
    "    tf.keras.layers.Dense(5, activation='selu', kernel_initializer='lecun_normal')\n",
    "])\n",
    "classifier = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(10, activation='selu', kernel_initializer='lecun_normal'), \n",
    "    tf.keras.layers.Dense(10, activation='selu', kernel_initializer='lecun_normal'),\n",
    "    tf.keras.layers.Dense(10, activation='selu', kernel_initializer='lecun_normal'), \n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model = tf.keras.models.Sequential([encoder, classifier])\n",
    "\n",
    "# 5) Training the source model\n",
    "lr = 0.01\n",
    "optimizer = tf.keras.optimizers.Adam(lr=lr)\n",
    "loss = 'binary_crossentropy'\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "patience = 100\n",
    "cw = {0:1, 1:10}\n",
    "callback = tf.keras.callbacks.EarlyStopping(patience=patience, restore_best_weights=True)\n",
    "history_source = model.fit(x=X_train, y=y_train, epochs=1000, batch_size=128,\n",
    "                                  validation_data=(X_valid, y_valid), callbacks=[callback], class_weight=cw)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Active learning via GP (RAVEN software as the oracle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Functions ######\n",
    "\n",
    "# Create Variational GP model\n",
    "class GPModel(ApproximateGP):\n",
    "    def __init__(self, inducing_points):\n",
    "        variational_distribution = CholeskyVariationalDistribution(inducing_points.size(0))\n",
    "        variational_strategy = VariationalStrategy(self, inducing_points, variational_distribution, learn_inducing_locations=True)\n",
    "        super(GPModel, self).__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# Evaluate the model\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    means = torch.tensor([0.])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in test_loader:\n",
    "            preds = model(x_batch)\n",
    "            means = torch.cat([means, preds.mean.cpu()])\n",
    "    means = means[1:]\n",
    "    means[means<0] = -1\n",
    "    means[means>0] = 1\n",
    "\n",
    "    metrics = []\n",
    "    for i in [balanced_accuracy_score, accuracy_score, roc_auc_score, f1_score]:\n",
    "        metrics.append(np.round(i(test_y, means),3))\n",
    "    return metrics\n",
    "\n",
    "# Quary the next point to be labeled based on acquisition functions\n",
    "def active_query(n):\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    means = torch.tensor([0.])\n",
    "    stds = torch.tensor([0.])\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in grid_loader:\n",
    "            preds = model(x_batch)\n",
    "            means = torch.cat([means, preds.mean.cpu()])\n",
    "            stds = torch.cat([stds, preds.stddev.cpu()])\n",
    "    means = means[1:]\n",
    "    stds = stds[1:]\n",
    "\n",
    "    # Acquisition functions (AFs) --> Refer to the paper for more info\n",
    "    \n",
    "    ## AF1:\n",
    "    #crit = (abs(means))/(torch.sqrt(1 + stds**2))\n",
    "    #arg_query = torch.argsort(crit)[:n]\n",
    "    \n",
    "    ## AF2:\n",
    "    #crit = stds**2\n",
    "    #arg_query = torch.argsort(crit, descending=True)[:n]\n",
    "    \n",
    "    ## AF3:\n",
    "    crit = abs(means)\n",
    "    arg_query = torch.argsort(crit)[:n]\n",
    "    \n",
    "    return arg_query\n",
    "\n",
    "# Generate inputs for RAVEN\n",
    "def generate_input(arg_query, scaler, Tool_Mapping, onehot_encoder):\n",
    "    inverse_norm = scaler.inverse_transform(data_processed[arg_query])\n",
    "    tool = onehot_encoder.inverse_transform(inverse_norm[:,:4])\n",
    "    return np.concatenate([tool.reshape(-1,1), inverse_norm[:,4:]], axis=1)\n",
    "\n",
    "# Run RAVEN software (oracle) to label the quaried data\n",
    "def RAVEN(data, n, m, resin, dire_update):\n",
    "    import time\n",
    "    import ravenAPI\n",
    "    \n",
    "    toolingMaterial, toolthicknessValue, partthicknessValue, topHtcValue, bottomHtcValue, ramp1, holdtemp, ramp2 = data\n",
    "    \n",
    "    if resin == 8552:\n",
    "        part_mat = 'COMPOSITE-HEXCEL-8552-AS4-PW-v2'\n",
    "    elif resin == 8551:\n",
    "        part_mat = 'COMPOSITE-HEXCEL-8551-7-AS4-Tape-v1'\n",
    "\n",
    "    # Create parametric input object\n",
    "    inputs = ravenAPI.ParametricInputs()\n",
    "\n",
    "    # Specify top and bottom HTCs (W/(m2 K))\n",
    "    inputs.addTopHTC(topHtcValue)\n",
    "    inputs.addBottomHTC(bottomHtcValue)\n",
    "\n",
    "    # Specify tooling facesheet thicknesses (meters)\n",
    "    inputs.addToolingThickness(toolthicknessValue/1000)\n",
    "\n",
    "    # Specify tooling materials (by unique name, which should be the filename without the extension)\n",
    "    for i in [toolingMaterial]:\n",
    "        inputs.addToolingMaterial(i)\n",
    "\n",
    "    # Create, populate, and add thermal cycles (C, C/min and min)\n",
    "    newCycle = ravenAPI.ThermalCycle('2Hold_ramp1_{}_hold_{}_ramp2_{}'.format(str(ramp1), str(holdtemp), str(ramp2)), 20)  # cycle name, initial temp\n",
    "    newCycle.addRamp(holdtemp, ramp1)  # target temp, heating rate\n",
    "    newCycle.addHold(60)  # hold time\n",
    "    newCycle.addRamp(180, ramp2)  # target temp, heating rate\n",
    "    newCycle.addHold(120)  # hold time\n",
    "    newCycle.addRamp(20, 3.5)  # target temp, heating rate\n",
    "    inputs.addThermalCycle(newCycle)\n",
    "\n",
    "    # Create, populate, and add material stacks\n",
    "    newStack = ravenAPI.MaterialStack('Part_{}'.format(str(partthicknessValue)))  # stack name\n",
    "    # material unique name, laminate thickness, initial DoC\n",
    "    newStack.addLaminate(part_mat, partthicknessValue/1000, 0.001)\n",
    "    inputs.addMaterialStack(newStack)\n",
    "\n",
    "    # Submit analysis\n",
    "    analysisID = ravenAPI.submitParametricAnalysis(inputs)\n",
    "    print(\"Started analysis: %s\" % analysisID)\n",
    "\n",
    "    # Wait for the analysis to complete\n",
    "    while ravenAPI.isRunning(analysisID):\n",
    "        time.sleep(1)\n",
    "\n",
    "    # Save results\n",
    "    direc = dire_update\n",
    "    ravenAPI.saveResultsCSV(analysisID,\n",
    "                            direc + 'sample_{}_AL{}.csv'.format(str(n), str(m)),\n",
    "                            fileFormat='hashable')\n",
    "    return print('sample_{} done'.format(str(n)))\n",
    "\n",
    "def load_main_data(dire, main, iter_n):\n",
    "    \"\"\":param\n",
    "    :param dire: directory of queried data from active learning procedure\n",
    "    :param query_num: query number (sequence)\n",
    "    :param main: main dataset so far (orig + data added from active learning)\n",
    "    \"\"\"\n",
    "    import os \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    #dire = r'C:\\Users\\miladrmz\\iCloudDrive\\PhD\\VarGP_TL_Paper\\Codes\\Test\\\\'\n",
    "    fileName = os.listdir(dire)\n",
    "    data = None\n",
    "    for i in fileName:\n",
    "        \n",
    "        if 'IN_' not in i:\n",
    "        \n",
    "            # read CSV file\n",
    "            raw = pd.read_csv(dire + i)\n",
    "\n",
    "            # Cure cycle specifications\n",
    "            ccSpec = pd.DataFrame(np.array([[j[5], j[9:12], j[22]] for j in raw['Cure_Cycle_Name']]),\n",
    "                                  columns=['Ramp1', 'Hold1_Temp', 'Ramp2'])\n",
    "\n",
    "            # Input/output variables\n",
    "            # Change units to mm so that it matches the rest of the code\n",
    "            raw[['Tool_Thickness', 'Part_Thickness']] = raw[['Tool_Thickness', 'Part_Thickness']]*1000\n",
    "\n",
    "            inptClmnName = ['Tool_Material', 'Tool_Thickness', 'Part_Thickness', 'Top_HTC', 'Bottom_HTC']\n",
    "            inps = raw[inptClmnName]\n",
    "            outps = raw[['Part_Max_Exotherm_Value','Part_Max_Lag_Value']]\n",
    "\n",
    "            # Concat cure cycle specs and input variables in one DF\n",
    "            temp_inp = pd.concat([inps,ccSpec], axis=1)\n",
    "            temp = pd.concat([temp_inp,outps], axis=1)\n",
    "            #temp = pd.concat([toolMat_df,temp_num], axis=1)\n",
    "\n",
    "            # Create Final data table\n",
    "            if data is None:\n",
    "                data = temp\n",
    "            else:\n",
    "                data = pd.concat([data,temp], axis = 0).reset_index(drop=True)\n",
    "                \n",
    "            os.rename(dire + i, dire + 'IN_' + i)\n",
    "\n",
    "    data.to_csv('8552_Main_Limited_iter{}.csv'.format(iter_n))\n",
    "\n",
    "    ## adding queried data to the main dataset\n",
    "    main_T = pd.concat([main,data], axis = 0).reset_index(drop=True)\n",
    "    \n",
    "    return main_T, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Generate the grid space and preprocess the data\n",
    "grid = grid_generator()\n",
    "data_processed, scaler, Tool_Mapping, onehot_encoder = grid_preprocess(grid)\n",
    "\n",
    "# 2) read limited target data\n",
    "path = '\\Data\\\\'\n",
    "filename = '8551_Limited_Sample.csv'\n",
    "\n",
    "# data: for training initial model\n",
    "# encoder: onehot encoder; currently no use\n",
    "# Main: raw dataset; query data will be added to this dataset\n",
    "Main = pd.read_csv(path + filename, index_col=0)\n",
    "data, encoder = loadData(Main,onehot_encoder)\n",
    "\n",
    "# 3) Specify test and sample size for splitting the dataset\n",
    "source = data.copy()  # Source dataset including inputs and outputs\n",
    "\n",
    "X_train_scaled_S, y_train_S, Scaler_S = dataprep_trainOnly(source, refScaler=scaler)\n",
    "\n",
    "# 4) Testset\n",
    "path = '\\Data\\\\'\n",
    "filename = '8551_Test_Sample.csv'\n",
    "Test = pd.read_csv(path + filename, index_col=0)\n",
    "test_data, encoder = loadDataCC2(Test,onehot_encoder)\n",
    "X_test_scalded_S, y_test_S, Scaler_S = dataprep_trainOnly(test_data, refScaler=scaler)\n",
    "\n",
    "# 5) Format normalized data to torch tensor\n",
    "train_x = torch.from_numpy(X_train_scaled_S).float()\n",
    "test_x = torch.from_numpy(X_test_scalded_S).float()\n",
    "\n",
    "train_y = torch.from_numpy(y_train_S).float()\n",
    "test_y = torch.from_numpy(y_test_S).float()\n",
    "\n",
    "grid_x = torch.from_numpy(data_processed).float()\n",
    "grid_y = torch.from_numpy(np.ones(grid_x.shape[0])).float()\n",
    "\n",
    "# 6) Prepare data for training/iterations\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train_dataset = TensorDataset(train_x, train_y)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(test_x, test_y)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "grid_dataset = TensorDataset(grid_x, grid_y)\n",
    "grid_loader = DataLoader(grid_dataset, batch_size=1024, shuffle=False)\n",
    "\n",
    "# 7) Active learning via GP\n",
    "n=40 # iteration\n",
    "\n",
    "encoder_categories = ['TOOLING-Aluminium-606x-Null-v1', 'TOOLING-Composite-AS4-v1','TOOLING-Steel-1020-Null-v1',\n",
    "                      'TOOLING-Invar-36-Null-v2']\n",
    "encoder_categories_lower = [i.lower() for i in encoder_categories]\n",
    "\n",
    "resin = 8551 # whether source or target - 8551, 8552\n",
    "\n",
    "evaluation = []\n",
    "for al in np.arange(1, n+1):\n",
    "\n",
    "    # 7.a) train the model\n",
    "    count = X_train_scaled_S.shape[0]\n",
    "    le = np.arange(0, count)\n",
    "    k = int(count/10)\n",
    "    ch = choices(le, k=k)\n",
    "    inducing_points = train_x[ch, :]\n",
    "    model = GPModel(inducing_points=inducing_points)\n",
    "    likelihood = gpytorch.likelihoods.BernoulliLikelihood()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        likelihood = likelihood.cuda()\n",
    "\n",
    "    num_epochs = 100\n",
    "\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    # We use SGD here, rather than Adam. Emperically, we find that SGD is better for variational regression\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': model.parameters()},\n",
    "        {'params': likelihood.parameters()},\n",
    "    ], lr=0.01)\n",
    "\n",
    "    # Our loss object. We're using the VariationalELBO\n",
    "    mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=train_y.size(0))\n",
    "\n",
    "    epochs_iter = tqdm.autonotebook.tqdm(range(num_epochs), desc=\"Epoch\")\n",
    "    for i in epochs_iter:\n",
    "        # Within each iteration, we will go over each minibatch of data\n",
    "        minibatch_iter = tqdm.autonotebook.tqdm(train_loader, desc=\"Minibatch\", leave=False)\n",
    "        for x_batch, y_batch in minibatch_iter:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x_batch)\n",
    "            loss = -mll(output, y_batch)\n",
    "            minibatch_iter.set_postfix(loss=loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "    # 7.b) Evaluate the model\n",
    "    evaluation.append(evaluate())\n",
    "    print(evaluate())\n",
    "    \n",
    "    # 7.c)Query data\n",
    "    #Find arg of n most uncertain poitns\n",
    "    n = 5\n",
    "    arg_query = active_query(n)\n",
    "    # Generating the input array to be queried \n",
    "    new_inp = generate_input(arg_query, scaler, Tool_Mapping, onehot_encoder)\n",
    "    # Reformat the tool type to be read by RAVEN properly\n",
    "    dict_tool = dict(zip(encoder_categories_lower, encoder_categories))\n",
    "    new_inp[:,0] = [dict_tool[i] for i in new_inp[:,0]]\n",
    "\n",
    "    # 7.d) RAVEN simulation\n",
    "    ravenAPI.HOST = 'localhost'\n",
    "    ravenAPI.PORT = 58009\n",
    "    for j in np.arange(new_inp.shape[0]):\n",
    "        RAVEN(new_inp[j], j, al, resin, dire_update)\n",
    "        \n",
    "    # 4) Update training dataset\n",
    "    Main, ext = load_main_data(dire_update, Main, al)\n",
    "    print(Main.shape)\n",
    "    data, encoder = loadDataCC2(Main,onehot_encoder)\n",
    "    X_train_scaled_S, y_train_S, Scaler_S = dataprep_trainOnly(data, refScaler=scaler)\n",
    "    train_x = torch.from_numpy(X_train_scaled_S).float()\n",
    "    train_y = torch.from_numpy(y_train_S).float()\n",
    "    train_dataset = TensorDataset(train_x, train_y)\n",
    "    if Main.shape[0] > 256:\n",
    "        train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "    elif Main.shape[0] > 128:\n",
    "        train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "    else:\n",
    "        train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) load Target data (provided by AL)\n",
    "path = '\\Data\\\\'\n",
    "filename = '8551_Limited_TrainingSet_Sampe.csv'\n",
    "Main_T = pd.read_csv(path + filename, index_col=0)\n",
    "data_T, encoder = loadDataCC2(Main_T,onehot_encoder)\n",
    "X_train_T, X_valid_T, y_train_T, y_valid_T, scaler_s = dataprep(data_T, 0.2,refScaler=scaler)\n",
    "\n",
    "# 2) TL with Freezing\n",
    "dict_hist = {}\n",
    "dict_preds = {}\n",
    "patience = 100\n",
    "lr=0.01\n",
    "optimizer = tf.keras.optimizers.Adam(lr=lr)\n",
    "callback = tf.keras.callbacks.EarlyStopping(patience=patience, restore_best_weights=True)\n",
    "for i in range(5):\n",
    "    model_TL_raw = tf.keras.models.load_model('Source_model.h5')\n",
    "    model_1 = tf.keras.models.Sequential(model_TL_raw.layers[1].layers[:-1])\n",
    "    model_1.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "    model_TL = tf.keras.models.Sequential([model_TL_raw.layers[0], model_1])\n",
    "    \n",
    "    # Step1: Train TL model with freezing\n",
    "    lr=0.01\n",
    "    model_TL.layers[0].trainable = False\n",
    "    for layer in model_TL.layers[1].layers[:-1]:\n",
    "        layer.trainable = False\n",
    "    model_TL.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    patience = 100\n",
    "    history_TL = model_TL.fit(x=X_train_T, y=y_train_T, epochs=10, batch_size=16,\n",
    "                                      validation_data=(X_valid_T, y_valid_T), callbacks=[callback])\n",
    "    \n",
    "    # Step2: unfreeze and train\n",
    "    lr = 0.001\n",
    "    model_TL.layers[0].trainable = True\n",
    "    for layer in model_TL.layers[1].layers[:-1]:\n",
    "        layer.trainable = True\n",
    "    model_TL.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    patience = 100\n",
    "    history_TL_2 = model_TL.fit(x=X_train_T, y=y_train_T, epochs=100, batch_size=16,\n",
    "                                      validation_data=(X_valid_T, y_valid_T), callbacks=[callback])\n",
    "    \n",
    "    # Step3: Generate accuracy dicts\n",
    "    dict_hist['acc{}'.format(str(i))] = history_TL.history['accuracy'] + history_TL_2.history['accuracy']\n",
    "    dict_hist['val{}'.format(str(i))] = history_TL.history['val_accuracy'] + history_TL_2.history['val_accuracy']\n",
    "    \n",
    "    # Step4: Generate predictions (for roc curve)\n",
    "    dict_preds[i] = model_TL.predict(X_test_T)\n",
    "    \n",
    "    # Step5: Performance metric resutls\n",
    "    if i ==0:\n",
    "        a_all = np.array(evaluate(model_TL, X_test_T, y_test_T))\n",
    "    else:\n",
    "        a_all = np.row_stack((a_all, evaluate(model_TL, X_test_T, y_test_T)))\n",
    "        \n",
    "mean_all = a_all.mean(axis=0)\n",
    "std_all = a_all.std(axis=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
